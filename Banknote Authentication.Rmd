---
title: "STAT 5600 Final Project"
author: "Eric McKinney"
date: "April 27, 2016"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

```{r, message=FALSE}
library(randomForest)
library(verification)
library(ada)
library(gbm)
library(rpart)
library(rpart.plot)
library(MASS)
library(caret)
library(knitr)
library(e1071)
library(utils)
source("../kappa and classsum.R")
library(corrplot)
```
## Motivation

With about 13 billion Euro banknotes in circulation among roughly 340 million people in 19 European countries, counterfeit Euros are unfortunately a common problem for businesses and individuals. About 19.9 million paper Euros (worth about 850,000) were removed from circulation in 2009 [1]. Detection of fake banknotes can be surprisingly difficult, regardless of the measures taken to prevent accurate counterfeiting. The following data was collected by Dr. Volker Lohweg and Dr. Helene Dörksen at Ostwestfalen-Lippe University of Applied Sciences in Lemgo, Germany. Their purpose in gathering the data is to develop an app that consumers could use to identify bogus Euros.

## The Data

The data was extracted from 1372 images that were taken from genuine and counterfeit Euro banknote-like specimens. For digitization, an industrial camera (usually used for print inspection) was used. A Wavelet Transform tool was used to extract features (the attributes) from the images.

### Attribute Information
1372 observations with 5 variables
\begin{enumerate}
\item Variance of Wavelet Transformed image (continuous) 
\item Skewness of Wavelet Transformed image (continuous) 
\item Kurtosis of Wavelet Transformed image (continuous) 
\item Entropy of image (continuous) 
\item Counterfeit Banknote (0 = genuine, 1 = counterfeit)
\end{enumerate}
There are no missing values in the data. 762 were genuine, and 610 were counterfeit. We assume that the observations were independent of each other.

```{r, knitr}
notes = read.csv("data_banknote_authentication.txt", header = FALSE)
names(notes) = c("WaveVar", "WaveSkew", "WaveKurt", "Entropy", "Counterfeit")

set.seed(123)
kable(notes[sample(nrow(notes), 10), ], digits = 4, caption = "A random sample of the data")
```

### Behavior of the Data

```{r, fig.align = "center", fig.height = 8, fig.width = 7}
pairs(Counterfeit ~ ., data = notes, main = "Scatterplot Matrix")
```

There seems to be some definite linear and non-linear associations between the predictors, especially between WaveSkew, WaveKurt, and Entropy. I decided to plot my predictors again, but this time letting the counterfeit's be red and the genuine's be blue.

```{r, fig.align = "center", fig.height = 9, fig.width = 7}
plot(notes[, c(1:4)], pch = ifelse(notes$Counterfeit == 1, 2, 1), col = ifelse(notes$Counterfeit == 1, rgb(1,0,0,0.5), rgb(0,0,1,0.5)))
```

There is some strong seperation happening! For example, at a given level of entropy, counterfeits have a smaller WaveSkew and WaveVar. Let's take a closer look.

```{r, fig.align = "center", fig.height = 10, fig.width = 7}
par(mfrow = c(3, 2))
plot(notes$WaveVar, notes$WaveSkew, pch = ifelse(notes$Counterfeit == 1, 2, 1), col = ifelse(notes$Counterfeit == 1, rgb(1,0,0), rgb(0,0,1)), xlab = "WaveVar", ylab = "WaveSkew")
legend("bottomright", pch = c(2, 1), col = c(rgb(1,0,0), rgb(0,0,1)), c("Counterfeit", "Genuine"), cex = 1.2)

plot(notes$WaveVar, notes$WaveKurt, pch = ifelse(notes$Counterfeit == 1, 2, 1), col = ifelse(notes$Counterfeit == 1, rgb(1,0,0), rgb(0,0,1)), xlab = "WaveVar", ylab = "WaveKurt")
legend("topright", pch = c(2, 1), col = c(rgb(1,0,0), rgb(0,0,1)), c("Counterfeit", "Genuine"), cex = 1.2)

plot(notes$WaveVar, notes$Entropy, pch = ifelse(notes$Counterfeit == 1, 2, 1), col = ifelse(notes$Counterfeit == 1, rgb(1,0,0), rgb(0,0,1)), xlab = "WaveVar", ylab = "Entropy")
legend("bottomright", pch = c(2, 1), col = c(rgb(1,0,0), rgb(0,0,1)), c("Counterfeit", "Genuine"), cex = 1.2)

plot(notes$WaveSkew, notes$WaveKurt, pch = ifelse(notes$Counterfeit == 1, 2, 1), col = ifelse(notes$Counterfeit == 1, rgb(1,0,0,0.5), rgb(0,0,1,0.5)), xlab = "WaveSkew", ylab = "WaveKurt")
legend("topright", pch = c(2, 1), col = c(rgb(1,0,0), rgb(0,0,1)), c("Counterfeit", "Genuine"), cex = .8)

plot(notes$WaveSkew, notes$Entropy, pch = ifelse(notes$Counterfeit == 1, 2, 1), col = ifelse(notes$Counterfeit == 1, rgb(1,0,0,0.5), rgb(0,0,1,0.5)), xlab = "WaveSkew", ylab = "Entropy")
legend("bottomleft", pch = c(2, 1), col = c(rgb(1,0,0), rgb(0,0,1)), c("Counterfeit", "Genuine"), cex = .8)

plot(notes$WaveKurt, notes$Entropy, pch = ifelse(notes$Counterfeit == 1, 2, 1), col = ifelse(notes$Counterfeit == 1, rgb(1,0,0,0.5), rgb(0,0,1,0.5)), xlab = "WaveKurt", ylab = "Entropy")
legend("bottomright", pch = c(2, 1), col = c(rgb(1,0,0), rgb(0,0,1)), c("Counterfeit", "Genuine"), cex = .8)
```

```{r, fig.align = "center", fig.height = 10, fig.width = 7}
par(mfrow = c(2, 2))
hist(notes$WaveVar[notes$Counterfeit == 1], col = rgb(1,0,0,0.5), xlim = c(-8, 8), ylim = c(0,170), main = "WaveVar Counterfeit vs. Genuine", xlab = "Wavelength Variance")
hist(notes$WaveVar[notes$Counterfeit == 0], col = rgb(0,0,1,0.5), add = TRUE)
box()
legend("topleft", fill = c(rgb(1,0,0,0.5), rgb(0,0,1,0.5), rgb(0.5,0,0.5, 0.8)), leg = c("Counterfeit", "Genuine", "Overlap"), cex = .8)

hist(notes$WaveSkew[notes$Counterfeit == 1], col = rgb(1,0,0,0.5), xlim = c(-15,15), ylim = c(0,150), main = "WaveSkew Counterfeit vs. Genuine", xlab = "Wavelength Variance")
hist(notes$WaveSkew[notes$Counterfeit == 0], col = rgb(0,0,1,0.5), add = TRUE)
box()
legend("topleft", fill = c(rgb(1,0,0,0.5), rgb(0,0,1,0.5), rgb(0.5,0,0.5, 0.8)), leg = c("Counterfeit", "Genuine", "Overlap"), cex = .8)

hist(notes$WaveKurt[notes$Counterfeit == 1], col = rgb(1,0,0,0.5), xlim = c(-7,20), ylim = c(0,150), main = "WaveKurt Counterfeit vs. Genuine", xlab = "Wavelength Variance")
hist(notes$WaveKurt[notes$Counterfeit == 0], col = rgb(0,0,1,0.5), add = TRUE)
box()
legend("topright", fill = c(rgb(1,0,0,0.5), rgb(0,0,1,0.5), rgb(0.5,0,0.5, 0.8)), leg = c("Counterfeit", "Genuine", "Overlap"), cex = .8)

hist(notes$Entropy[notes$Counterfeit == 1], col = rgb(1,0,0,0.5), xlim = c(-9,3), ylim = c(0,200), main = "Entropy Counterfeit vs. Genuine", xlab = "Wavelength Variance")
hist(notes$Entropy[notes$Counterfeit == 0], col = rgb(0,0,1,0.5), add = TRUE)
box()
legend("topleft", fill = c(rgb(1,0,0,0.5), rgb(0,0,1,0.5), rgb(0.5,0,0.5, 0.8)), leg = c("Counterfeit", "Genuine", "Overlap"), cex = .8)
```

First in the scatterplots, while there is a lot of overlap between Entropy and WaveKurt, only counterfeits have WaveKurt greater than about 8.5 for any value of WaveVar. This is the same for any amount of Entropy too. Also, Counterfeits generally have smaller WaveSkew as compared to the genuine Euros.

Secondly, the histograms show a noticable difference in WaveVar distribution between real and fake Euros. However, the distribution of WaveVar is almost identical between the genuine and counterfeit.

## Multivariate Analysis

My goal is to identify accurate classification techniques that give us a high percentage correctly classified (PCC). However, two mistakes could occur when attempting to classify a banknote. We may mistakenly classify a counterfeit banknote as being genuine, or we might accidentally label a genuine banknote as being counterfeit. Specificity of a classification technique is the percentage of genuine banknotes correctly classified. Sensitivity is the percentage of counterfeits correctly classified.

I chose to first split my data into a testing and a training set. I randomly assigned 915 observations into my testing set. This is about 2/3 of the data. The remaining 1/3 is my training set.

```{r, echo=TRUE}
set.seed(124)
smplobs = sample(nrow(notes), 915)
trainNotes = notes[smplobs, ]
testNotes = notes[-smplobs, ]
```

I will measure the effectiveness of classification methods by comparing resubstitution PCC, 10-fold PCC on the training set, and test set PCC.

I began by applying LDA to the training set.


### Linear Discriminant Analysis

```{r, echo=TRUE}
Counterfeit.lda = lda(Counterfeit ~ ., data = trainNotes)

table(trainNotes$Counterfeit, predict(Counterfeit.lda)$class)
class.sum(trainNotes$Counterfeit, predict(Counterfeit.lda)$posterior[,2])
ldarsb = class.sum(trainNotes$Counterfeit, predict(Counterfeit.lda)$posterior[,2])[1,2]

Counterfeit.lda.xval = rep(0, nrow(trainNotes))
xvs = rep(1:10, length = nrow(trainNotes))
xvs = sample(xvs)
for (i in 1:10) {
  test = trainNotes[xvs == i,]
  train = trainNotes[xvs != i,]
  glub = lda(Counterfeit ~ ., data = train)
  Counterfeit.lda.xval[xvs == i] = predict(glub, test)$class
}

table(trainNotes$Counterfeit, Counterfeit.lda.xval)
class.sum(trainNotes$Counterfeit, Counterfeit.lda.xval)
ldafld = class.sum(trainNotes$Counterfeit, Counterfeit.lda.xval)[1,2]

table(testNotes$Counterfeit, predict(Counterfeit.lda, testNotes)$class)
class.sum(testNotes$Counterfeit, predict(Counterfeit.lda, testNotes)$posterior[,2])
ldatst = class.sum(testNotes$Counterfeit, predict(Counterfeit.lda, testNotes)$posterior[,2])[1, 2]
```

LDA gives us good results. While 8 genuines were misclassified, all of the counterfeits were caught! Although the measured variables are not quite multivariate normal as seen in the following normal QQ plots, and the covariance matrices for the groups differ, we accept these results as being good. The true measure of usefulness in a model is in how it performs.

```{r, fig.align = "center", fig.height = 7}
par(mfrow = c(2, 2))
qqnorm(trainNotes$WaveVar, main="WaveVar Normal QQ Plot")
qqline(trainNotes$WaveVar, col = 2)
qqnorm(trainNotes$WaveSkew, main="WaveSkew Normal QQ Plot")
qqline(trainNotes$WaveSkew, col = 2)
qqnorm(trainNotes$WaveKurt, main="WaveKurt Normal QQ Plot")
qqline(trainNotes$WaveKurt, col = 2)
qqnorm(trainNotes$Entropy, main="Entropy Normal QQ Plot")
qqline(trainNotes$Entropy, col = 2)

trainNotesC = trainNotes[trainNotes$Counterfeit == 1, -5]
trainNotesG = trainNotes[trainNotes$Counterfeit == 0, -5]

kable(cov(trainNotesC), digits = 4, caption = "Covariance Matrix for Counterfeit Group")
kable(cov(trainNotesG), digits = 4, caption = "Covariance Matrix for Genuine Group")
```


### Quadratic Discriminant Analysis

```{r, echo=TRUE}
Counterfeit.qda = qda(Counterfeit ~ ., data = trainNotes)

table(trainNotes$Counterfeit, predict(Counterfeit.qda)$class)
class.sum(trainNotes$Counterfeit, predict(Counterfeit.qda)$posterior[, 2])
qdarsb = class.sum(trainNotes$Counterfeit, predict(Counterfeit.qda)$posterior[, 2])[1, 2]

Counterfeit.qda.xval = rep(0, nrow(trainNotes))
xvs = rep(1:10, length = nrow(trainNotes))
xvs = sample(xvs)
for (i in 1:10) {
  test = trainNotes[xvs == i, ]
  train = trainNotes[xvs != i, ]
  glub = qda(Counterfeit ~ ., data = train)
  Counterfeit.qda.xval[xvs == i] = predict(glub, test)$class
}

table(trainNotes$Counterfeit,Counterfeit.qda.xval)
class.sum(trainNotes$Counterfeit, Counterfeit.qda.xval)
qdafld = class.sum(trainNotes$Counterfeit, Counterfeit.qda.xval)[1, 2]

table(testNotes$Counterfeit, predict(Counterfeit.qda, testNotes)$class)
class.sum(testNotes$Counterfeit, predict(Counterfeit.qda, testNotes)$posterior[,2])
qdatst = class.sum(testNotes$Counterfeit, predict(Counterfeit.qda, testNotes)$posterior[,2])[1, 2]
```

QDA performed very well too, and similarly correctly classified all counterfeits. Both QDA and LDA have Cohen's Kappa close to 1, as well as AUC's close to 1. This suggests that the models are a good fit to the data.


### Logistic Regression

While logistic regression is only appropriate for two class models, this is what we need for our data. I suspect that Logistic Regression will outperform LDA and QDA, since it does not have assumptions about the distributions of the predictors that need to be met.

```{r, warning=FALSE, echo=TRUE}
Counterfeit.lr = glm(Counterfeit ~ ., family = binomial, data = trainNotes)

table(trainNotes$Counterfeit, round(predict(Counterfeit.lr, type = "response") + 0.0000001))
class.sum(trainNotes$Counterfeit, predict(Counterfeit.lr, type = "response"))
lrrsb = class.sum(trainNotes$Counterfeit, predict(Counterfeit.lr, type = "response"))[1, 2]

Counterfeit.lr.xval = rep(0, length = nrow(trainNotes))
xvs = rep(1:10, length = nrow(trainNotes))
xvs = sample(xvs)
for (i in 1:10) {
  test = trainNotes[xvs == i, ]
  train = trainNotes[xvs != i, ]
  glub = glm(Counterfeit ~ ., family = binomial, data = train)
  Counterfeit.lr.xval[xvs == i] = predict(glub, test, type="response")
}

table(trainNotes$Counterfeit, round(Counterfeit.lr.xval))
class.sum(trainNotes$Counterfeit, Counterfeit.lr.xval)
lrfld = class.sum(trainNotes$Counterfeit, Counterfeit.lr.xval)[1, 2]

table(testNotes$Counterfeit, round(predict(Counterfeit.lr, testNotes, type = "response") + 0.0000001))
class.sum(testNotes$Counterfeit, predict(Counterfeit.lr, testNotes, type = "response"))
lrtst = class.sum(testNotes$Counterfeit, predict(Counterfeit.lr, testNotes, type = "response"))[1, 2]
```

While Logistic Regression performed slightly better on the training data's 10-fold cross validation, it didn't perform better on the testing data set. It did better at correctly classifying genuine Euro's, but at the cost of incorrectly classifying a few counterfeits. However, it is still correctly classifying over 98% of the banknotes.


### Nearest Neighbor Classification

I ran knn3 for k = 2, 3, 4, 5, 6, 7, 8, and 9 nearest neighbors.
Several k's give us a perfect 10-fold cross validated classification including k = 4.

```{r, warning=FALSE, echo=TRUE}
nNeighbors = 4
Counterfeit.knn = knn3(Counterfeit ~ ., data = trainNotes, k = nNeighbors)

# Resubstitution confusion matrix and accuracies.
table(trainNotes$Counterfeit, round(predict(Counterfeit.knn, trainNotes, type = "prob")[, 2]))
class.sum(trainNotes$Counterfeit, round(predict(Counterfeit.knn, trainNotes, type = "prob")[, 2]))
knnrsb = class.sum(trainNotes$Counterfeit, round(predict(Counterfeit.knn, trainNotes, type = "prob")[, 2]))[1, 2]

Counterfeit.knn.xval = rep(0, length = nrow(trainNotes))
xvs = rep(c(1:10), length = nrow(trainNotes))
xvs = sample(xvs)
for (i in 1:10) {
  train = trainNotes[xvs != i, ]
  test = trainNotes[xvs == i, ]
  glub = knn3(Counterfeit ~ ., data = train, k = nNeighbors)
  Counterfeit.knn.xval[xvs == i] = predict(glub, test, type = "prob")[, 2]
}

table(trainNotes$Counterfeit, round(Counterfeit.knn.xval))
class.sum(trainNotes$Counterfeit, round(Counterfeit.knn.xval))
knnfld = class.sum(trainNotes$Counterfeit, round(Counterfeit.knn.xval))[1, 2]

table(testNotes$Counterfeit, round(predict(Counterfeit.knn, testNotes, type = "prob")[, 2]))
class.sum(testNotes$Counterfeit, round(predict(Counterfeit.knn, testNotes, type = "prob")[, 2]))
knntst = class.sum(testNotes$Counterfeit, round(predict(Counterfeit.knn, testNotes, type = "prob")[, 2]))[1, 2]
```

4-nearest neighbors also gave us a perfect PCC for our test set too! This seems to be a great model for our data.


### Single Classification Tree

```{r, fig.align = "center", fig.height = 9, echo=TRUE}
Counterfeit.rpartfull = rpart(Counterfeit ~ ., method = "class", control = rpart.control(cp = 0.0, minsplit = 2), data = trainNotes)
par(mfcol = c(2, 1))
plot(Counterfeit.rpartfull, main = "Fully Grown Tree")
plotcp(Counterfeit.rpartfull)
```

```{r, fig.align = "center", fig.height = 9, echo=TRUE}
Counterfeit.rpartCP0044 = rpart(Counterfeit ~ ., method = "class", control=rpart.control(cp=0.0044), data = trainNotes)
prp(Counterfeit.rpartCP0044, Margin = 0.1, varlen = 0, extra = 1, tweak = .9)
# Counterfeit.rpartCP0044

table(trainNotes$Counterfeit, predict(Counterfeit.rpartCP0044, type = "class"))
class.sum(trainNotes$Counterfeit, predict(Counterfeit.rpartCP0044, type = "prob")[, 2])
crtrsb = class.sum(trainNotes$Counterfeit, predict(Counterfeit.rpartCP0044, type = "prob")[, 2])[1, 2]

Counterfeit.rpartCP0044.xval = rep(0, length(nrow(trainNotes)))
xvs = rep(c(1:10), length = nrow(trainNotes))
xvs = sample(xvs)
for(i in 1:10) {
  train = trainNotes[xvs != i, ]
  test = trainNotes[xvs == i, ]
  rp = rpart(Counterfeit ~ ., method = "class", data = train, control = rpart.control(cp=0.0044))
  Counterfeit.rpartCP0044.xval[xvs == i] = predict(rp, test, type = "prob")[, 2]
}

table(trainNotes$Counterfeit, round(Counterfeit.rpartCP0044.xval))
class.sum(trainNotes$Counterfeit, Counterfeit.rpartCP0044.xval)
crtfld = class.sum(trainNotes$Counterfeit, Counterfeit.rpartCP0044.xval)[1, 2]

table(testNotes$Counterfeit, predict(Counterfeit.rpartCP0044, testNotes, type = "class"))
class.sum(testNotes$Counterfeit, predict(Counterfeit.rpartCP0044, testNotes, type = "prob")[, 2])
crttst = class.sum(testNotes$Counterfeit, predict(Counterfeit.rpartCP0044, testNotes, type = "prob")[, 2])[1, 2]
```

By using the 1-SE rule we trim our fully grown tree back to avoid modeling noise. We do this using a cp value of 0.0044. Since the first split was done on WaveVar, this gives a level of importance to that predictor in the classification. In fact, WaveVar and WaveKurt make up the second splits and the third splits. Again, this method is getting in the upper ninties for PCC. However, it is the worst technique as far as 10-fold crossvalidation and prediction on the test data.

Let's try a random forest method.

### Random Forests

```{r, echo=TRUE}
Counterfeit.rf = randomForest(as.factor(Counterfeit) ~ ., importance = TRUE, keep.forest = TRUE, data = trainNotes)

Counterfeit.rf$confusion
class.sum(trainNotes$Counterfeit, predict(Counterfeit.rf, type = "prob")[, 2])
rfrsb = class.sum(trainNotes$Counterfeit, predict(Counterfeit.rf, type = "prob")[, 2])[1, 2]

Counterfeit.rf.xval.class = rep(0,length = nrow(trainNotes))
Counterfeit.rf.xval.prob = rep(0,length = nrow(trainNotes))
xvs = rep(1:10, length = nrow(trainNotes))
xvs = sample(xvs)
for(i in 1:10){
    train = trainNotes[xvs != i, ]
    test = trainNotes[xvs == i, ]
    glub = randomForest(as.factor(Counterfeit) ~ ., data = train)
    Counterfeit.rf.xval.class[xvs == i] = predict(glub, test, type = "response")
    Counterfeit.rf.xval.prob[xvs==i] = predict(glub, test, type = "prob")[, 2]
}

table(trainNotes$Counterfeit, Counterfeit.rf.xval.class)
class.sum(trainNotes$Counterfeit, Counterfeit.rf.xval.prob)
rffld = class.sum(trainNotes$Counterfeit, Counterfeit.rf.xval.prob)[1, 2]

table(testNotes$Counterfeit, predict(Counterfeit.rf, testNotes, type = "response"))
class.sum(testNotes$Counterfeit, round(predict(Counterfeit.rf, testNotes, type = "prob")[, 2]))
rftst = class.sum(testNotes$Counterfeit, round(predict(Counterfeit.rf, testNotes, type = "prob")[, 2]))[1, 2]
```



```{r, fig.align = "center", fig.height = 8}
varImpPlot(Counterfeit.rf)
```

We also see from random forest's variable imporatance plot that WaveVar and WaveSkew are the two most important variables. WaveVar seems to be contributing a significant amount of information into the classification over the others. It's not surprising if you'll remember the histogram of WaveVar. There was more seperation in the distributions of WaveVar for genuines vs. counterfeits than any of the other predictors.

```{r, fig.align = "center", fig.height = 8}
par(mfrow = c(2,2))
partialPlot(Counterfeit.rf, trainNotes, WaveVar, which.class = 1, main = "")
partialPlot(Counterfeit.rf, trainNotes, WaveSkew, which.class = 1, main = "")
partialPlot(Counterfeit.rf, trainNotes, WaveKurt, which.class = 1, main = "")
partialPlot(Counterfeit.rf, trainNotes, Entropy, which.class = 1, main = "")
```

From the partial dependence plots we see that very few fake Euro's can produce a WaveVar above 3. Similarly, very few imposters have a WaveSkew above 5. This is also seen in the single tree that was plotted before.

### AdaBoost

```{r, echo=TRUE}
Counterfeit.ada = ada(Counterfeit ~ ., data = trainNotes, loss = "exponential")

Counterfeit.ada$confusion
class.sum(trainNotes$Counterfeit, predict(Counterfeit.ada, trainNotes, type = "prob")[, 2])
adarsb = class.sum(trainNotes$Counterfeit, predict(Counterfeit.ada, trainNotes, type = "prob")[, 2])[1, 2]

Counterfeit.ada.xval.class = rep(0, length = nrow(trainNotes))
Counterfeit.ada.xval.prob = rep(0, length = nrow(trainNotes))
xvs = rep(1:10, length = nrow(trainNotes))
xvs = sample(xvs)
for (i in 1:10) {
    train = trainNotes[xvs != i, ]
    test = trainNotes[xvs == i, ]
    glub = ada(as.factor(Counterfeit) ~ ., data = train, loss = "exponential")
    Counterfeit.ada.xval.class[xvs == i] = predict(glub, test, type = "vector")
    Counterfeit.ada.xval.prob[xvs == i] = predict(glub, test, type = "probs")[, 2]
}

table(trainNotes$Counterfeit, Counterfeit.ada.xval.class)
class.sum(trainNotes$Counterfeit, Counterfeit.ada.xval.prob)
adafld = as.numeric(class.sum(trainNotes$Counterfeit, Counterfeit.ada.xval.prob)[1, 2])

table(testNotes$Counterfeit, round(predict(Counterfeit.ada, testNotes, type = "prob")[, 2]))
class.sum(testNotes$Counterfeit, predict(Counterfeit.ada, testNotes, type = "prob")[, 2])
adatst = class.sum(testNotes$Counterfeit, predict(Counterfeit.ada, testNotes, type = "prob")[, 2])[1, 2]
```


### Support Vector Machines

```{r, warning=FALSE, echo=TRUE}
Counterfeit.svm = svm(as.factor(Counterfeit) ~ ., probability = TRUE, data = trainNotes)

Counterfeit.svm.resubpred = predict(Counterfeit.svm, trainNotes, probability = TRUE)
table(trainNotes$Counterfeit, round(attr(Counterfeit.svm.resubpred, "probabilities")[, 2]))
class.sum(trainNotes$Counterfeit, attr(Counterfeit.svm.resubpred, "probabilities")[, 2])
svmrsb = class.sum(trainNotes$Counterfeit, attr(Counterfeit.svm.resubpred, "probabilities")[, 2])[1, 2]

Counterfeit.svm.xvalpred = rep(0, nrow(trainNotes))
xvs = rep(1:10, length = nrow(trainNotes))
xvs = sample(xvs)
for (i in 1:10) {
      train = trainNotes[xvs != i, ]
      test = trainNotes[xvs == i, ]
      glub = svm(as.factor(Counterfeit) ~ ., probability = TRUE, data = train)
      Counterfeit.svm.xvalpred[xvs == i] = attr(predict(glub, test, probability = TRUE), "probabilities")[, 2]
}

table(trainNotes$Counterfeit, round(Counterfeit.svm.xvalpred))
class.sum(trainNotes$Counterfeit, Counterfeit.svm.xvalpred)
svmfld = class.sum(trainNotes$Counterfeit, Counterfeit.svm.xvalpred)[1, 2]

table(testNotes$Counterfeit, round(attr(predict(Counterfeit.svm, testNotes, probability = TRUE), "probabilities")[, 2]))
class.sum(testNotes$Counterfeit, attr(predict(Counterfeit.svm, testNotes, probability = TRUE), "probabilities")[, 2])
svmtst = class.sum(testNotes$Counterfeit, attr(predict(Counterfeit.svm, testNotes, probability = TRUE), "probabilities")[, 2])[1, 2]
```

### Gradient Boosted Trees

```{r, warning=FALSE, echo=TRUE}
Counterfeit.gbm = gbm(Counterfeit ~ ., distribution = "bernoulli", n.trees = 100, interaction.depth = 1, shrinkage = 0.01, data = trainNotes)

table(trainNotes$Counterfeit, round(predict(Counterfeit.gbm, type = "response", n.trees = 100, interaction.depth = 1, shrinkage = 0.01) + 0.0000001))
class.sum(trainNotes$Counterfeit, predict(Counterfeit.gbm, type = "response", n.trees = 100, interaction.depth = 1, shrinkage = 0.01))
gbmrsb = class.sum(trainNotes$Counterfeit, predict(Counterfeit.gbm, type = "response", n.trees = 100, interaction.depth = 1, shrinkage = 0.01))[1, 2]

Counterfeit.gbm.xvalpr = rep(0, nrow(trainNotes))
xvs = rep(1:10, length = nrow(trainNotes))
xvs = sample(xvs)
for (i in 1:10) {
      train = trainNotes[xvs != i, ]
      test = trainNotes[xvs == i, ]
      glub = gbm(Counterfeit ~ ., distribution = "bernoulli", n.trees = 100, interaction.depth = 1, shrinkage = 0.01, data = train)
      Counterfeit.gbm.xvalpr[xvs == i] = predict(glub, newdata = test, type = "response", n.trees = 100, interaction.depth = 1, shrinkage = 0.01)
}

table(trainNotes$Counterfeit, round(Counterfeit.gbm.xvalpr + 0.0000001))
class.sum(trainNotes$Counterfeit, Counterfeit.gbm.xvalpr)
gbmfld = class.sum(trainNotes$Counterfeit, Counterfeit.gbm.xvalpr)[1, 2]

table(testNotes$Counterfeit, round(predict(Counterfeit.gbm, testNotes, type = "response", n.trees = 100, interaction.depth = 1, shrinkage = 0.01) + 0.0000001))
class.sum(testNotes$Counterfeit, predict(Counterfeit.gbm, testNotes, type = "response", n.trees = 100, interaction.depth = 1, shrinkage = 0.01))
gbmtst = class.sum(testNotes$Counterfeit, predict(Counterfeit.gbm, testNotes, type = "response", n.trees = 100, interaction.depth = 1, shrinkage = 0.01))[1, 2]
```

Not surprisingly, the untuned gbm did poorer than the others. Interestingly though, it misclassified about the same genuines as it did counterfeits.

#### Tuning the gbm

```{r, eval=FALSE, echo=TRUE}
fitControl = trainControl(method = "cv", number = 10)
gbmGridNotes = expand.grid(interaction.depth = c(12, 14, 16, 18, 20), n.trees = c(25, 50, 75, 100), shrinkage = c(0.01, 0.05, 0.1, 0.2), n.minobsinnode = 10)
gbmFitNotes = train(as.factor(Counterfeit) ~ ., method = "gbm", tuneGrid = gbmGridNotes, trControl = fitControl, data = trainNotes)
gbmFitNotes

gbmGridNotes = expand.grid(interaction.depth = c(19, 20, 21, 24), n.trees = c(40, 50, 60), shrinkage = c(0.1, 0.2, 0.3), n.minobsinnode = 10)
gbmFitNotes = train(as.factor(Counterfeit) ~ ., method = "gbm", tuneGrid = gbmGridNotes, trControl = fitControl, data = trainNotes)
gbmFitNotes

gbmGridNotes = expand.grid(interaction.depth = c(20), n.trees = c(55, 60, 70, 80), shrinkage = c(0.3, 0.4, 0.5, 0.6), n.minobsinnode = 10)
gbmFitNotes = train(as.factor(Counterfeit) ~ ., method = "gbm", tuneGrid = gbmGridNotes, trControl = fitControl, data = trainNotes)
gbmFitNotes

gbmGridNotes = expand.grid(interaction.depth = c(16), n.trees = c(65, 70, 75), shrinkage = c(0.6, 0.7, 0.8, 0.9), n.minobsinnode = 10)
gbmFitNotes = train(as.factor(Counterfeit) ~ ., method = "gbm", tuneGrid = gbmGridNotes, trControl = fitControl, data = trainNotes)
gbmFitNotes

gbmGridNotes = expand.grid(interaction.depth = c(16), n.trees = c(65), shrinkage = c(0.75, 0.8, 0.85), n.minobsinnode = 10)
gbmFitNotes = train(as.factor(Counterfeit) ~ ., method = "gbm", tuneGrid = gbmGridNotes, trControl = fitControl, data = trainNotes)
gbmFitNotes
```

After tuning the gradient boosted machine I found that the optimal values were number of trees = 65, interaction depth = 16, and shrinkage = 0.85.

### Tuned Gradient Boosted Machine

```{r, warning=FALSE, echo=TRUE}
Counterfeit.tgbm = gbm(Counterfeit ~ ., distribution = "bernoulli", n.trees = 65, interaction.depth = 16, shrinkage = 0.85, data = trainNotes)

table(trainNotes$Counterfeit, round(predict(Counterfeit.tgbm, type = "response", n.trees = 65, interaction.depth = 16, shrinkage = 0.85) + 0.0000001))
class.sum(trainNotes$Counterfeit, predict(Counterfeit.tgbm, type = "response", n.trees = 65, interaction.depth = 16, shrinkage = 0.85))
tgbmrsb = class.sum(trainNotes$Counterfeit, predict(Counterfeit.tgbm, type = "response", n.trees = 65, interaction.depth = 16, shrinkage = 0.85))[1, 2]

Counterfeit.tgbm.xvalpr = rep(0, nrow(trainNotes))
xvs = rep(1:10, length = nrow(trainNotes))
xvs = sample(xvs)
for (i in 1:10) {
  train = trainNotes[xvs != i, ]
  test = trainNotes[xvs == i, ]
  glub = gbm(Counterfeit ~ ., distribution = "bernoulli", n.trees = 65, interaction.depth = 16, shrinkage = 0.85, data = train)
  Counterfeit.tgbm.xvalpr[xvs == i] = predict(glub, newdata = test, type = "response", n.trees = 65, interaction.depth = 16, shrinkage = 0.85)
}

table(trainNotes$Counterfeit, round(Counterfeit.tgbm.xvalpr + 0.0000001))
class.sum(trainNotes$Counterfeit, Counterfeit.tgbm.xvalpr)
tgbmfld = class.sum(trainNotes$Counterfeit, Counterfeit.tgbm.xvalpr)[1, 2]

table(testNotes$Counterfeit, round(predict(Counterfeit.tgbm, testNotes, type = "response", n.trees = 65, interaction.depth = 16, shrinkage = 0.85) + 0.0000001))
class.sum(testNotes$Counterfeit, predict(Counterfeit.tgbm, testNotes, type = "response", n.trees = 65, interaction.depth = 16, shrinkage = 0.85))
tgbmtst = class.sum(testNotes$Counterfeit, predict(Counterfeit.tgbm, testNotes, type = "response", n.trees = 65, interaction.depth = 16, shrinkage = 0.85))[1, 2]
```

Wow! The tuning made a huge difference! The classification has improved by about 10 to 15%.


## Conclusions

```{r, kable, echo = FALSE}
xvalTabl <- matrix(c(ldarsb, ldafld, ldatst, qdarsb, qdafld, qdatst, lrrsb, lrfld, lrtst, knnrsb, knnfld, knntst, crtrsb, crtfld, crttst, rfrsb, rffld, rftst, adarsb, adafld, adatst, svmrsb, svmfld, svmtst, gbmrsb, gbmfld, gbmtst, tgbmrsb, tgbmfld, tgbmtst), ncol = 3, byrow = TRUE)
colnames(xvalTabl) <- c("Resubstitution", "10-fold", "Test data")
rownames(xvalTabl) <- c("LDA", "QDA", "Logistic Regression", "Nearest Neighbor", "Classification Tree", "Random Forests", "Adaboosted Trees", "Support Vector Machines", "Gradient Boosted Trees", "Tuned Gradient Boosted Trees")
xvalTabl <- as.table(xvalTabl)
kable(xvalTabl, digits = 2, caption = "Cross Validated PCC's")
# align = "c",
```

In conclusion, all of the techniques did very well classifying the groups of banknotes. Even the "less suffisticated" techniques such as QDA performed just as well as the more modern algorithms such as tuned gradient boosted machines. Surprisingly, support vector machines did a perfect job even without any tuning. It correctly classified all banknotes in the 10-fold crossvalidation, and in the test data set! The 4 nearest neighbors did too! The rest of the techniques were not far behind though--only misclassifying a handful of notes. Overall, it seemed like the data was well behaved, and we had the sufficient information in our predictors to accurately classify genuine and counterfeit Euro banknotes. I would suggest using either support vector machines, or 4 nearest neighbors.

### References

[1] Michel Beuret, "Les mystères de la fausse monnaie", Allez savoir!, no. 50, May 2011.

[2] R Core Team (2016). R: A language and environment for statistical computing. R Foundation for Statistical Computing, Vienna, Austria. URL https://www.R-project.org/.